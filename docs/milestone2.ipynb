{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "## CS207 Final Project, Group 28\n",
    "#### Team Members:\n",
    "* Josh Bodner  \n",
    "* ThÃ©o Guenais  \n",
    "* Daiki Ina  \n",
    "* Junzhi Gong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ever since the notion of a derivative was first defined in the days of Newton and Leibniz, differentiation has become a crucial component across quantitative analyses. Today, differentiation is applied within a wide range of scientific disciplines from cell biology to electrical engineering and astrophysics. Differentiation is also central to the domain of optimization where it is applied to find maximum and minimum values of functions.\n",
    "\n",
    "Because of the widespread use and applicability of differentiation, it would be highly beneficial if scientists could efficiently calculate derivatives of functions using a Python package. Such a package could save scientists time and energy from having to compute derivatives symbolically, which can be especially complicated if the function of interest has vector inputs and outputs.\n",
    "\n",
    "Our Python package, autodiff, addresses this need by allowing the user to implement the forward mode of automatic differentiation (AD). Using AD, we are able to calculate derivatives to machine precision in a manner that is less costly than symbolic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In this section we provide a brief overview of the mathematical concepts relevant to our implementation:\n",
    "\n",
    "##### 1. Multivariable Differental Calculus:\n",
    "\n",
    "Let $f: {\\rm R^m} \\rightarrow {\\rm R^n}: x \\mapsto f(x)$ . Under certain regularity and smoothness assumptions, we define the derivative of f as $ f': {\\rm R^m} \\rightarrow L_{{\\rm R^m}, {\\rm R^n}}$, with $ L_{{\\rm R^m}, {\\rm R^n}}$ being the space of the linear mapping $ {\\rm R^m}\\rightarrow {\\rm R^n}$ which can be transformed to $ {\\rm R^{mxn}}$.\n",
    "\n",
    "*This general definition might be broken due to lack of smoothness, in which case, we refer to \"directional derivatives\" and specifically \"Gateaux-derivatives\".*\n",
    "\n",
    "From this lense, we can understand the derivative of a function evaluated at a given point to be a matrix. When this function is real-valued, we can obtain a vector $ \\nabla_x f \\in {\\rm R^m} $.\n",
    "\n",
    "Specifically, the gradient of a scalar-valued multivariable function is a vector of its partial derivatives with respect to each input:\n",
    "\n",
    "$\\nabla f = \n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_3} \\\\\n",
    "    \\vdots\n",
    "  \\end{bmatrix}$\n",
    "\n",
    "##### 2. Chain Rule\n",
    "Let $ f: {\\rm R^m} \\rightarrow {\\rm R^n} $ and $ g: {\\rm R^p} \\rightarrow {\\rm R^m} $. \n",
    "Then $ f \\circ g: {\\rm R^p} \\rightarrow {\\rm R^n} $ is such that (under regularity assumptions), $ (f \\circ g)'(x) = f'(g(x)) \\cdot g'(x) $. This operational rule, known as the chain rule, is a crucial part of AD.\n",
    "\n",
    "---\n",
    "\n",
    "*Example:*  \n",
    "$ f(x) = sin(x)$  \n",
    "$ g(x) = e^{x}$  \n",
    "$ (f \\circ g)(x) = sin(e^{x})$  \n",
    " \n",
    "$(f \\circ g)'(x) = f'(g(x))g'(x) = cos(e^{x})e^{x}$\n",
    "\n",
    "##### 3. Automatic Differentiation\n",
    "\n",
    "*Automatic Differentiation and Forward Mode:* \n",
    "\n",
    "Automatic differentiation is a process for evaluating derivatives computationally at a particular evaluation point. Specifically, the forward mode of automatic differentiation calculates the product of the gradient with a seed vector $ \\nabla f \\cdot p $ or the product of the Jacobian with a seed vector ($Jp$) if $f$ is a vector.\n",
    "\n",
    "In the forward mode of automatic differentiation, the calculation is done in steps where the partial derivatives and values are computed at each step of the computational graph. The edges of the computational graph are such that transitioning from node to node involves simple operations or calculation of elementary functions such as $sin(x)$, $cos(x)$, $e^{x}$, etc.\n",
    "\n",
    "*Precision and Efficiency:*\n",
    "\n",
    "Automatic differentiation is able to evaluate derivatives to machine precision. This is because it computes exact derivatives using the component elementary functions at each step of the computational graph. The precision of automatic differentiation is one of its advantages over other methods such as numerical differentiation, which can suffer from floating point precision errors.\n",
    "\n",
    "The forward mode of automatic differentiation is also particularly efficient when the the number of functions to evaluate is much larger than the number of independent variables $ n >> m$  for $f: {\\rm R^m} \\rightarrow {\\rm R^n}$. This is because forward mode involves performing one sweep over the computational graph for each of the m independent variables. In comparison, the reverse mode of automatic differentiation requires n sweeps over the computational graph and is thus more efficient when $m >> n$.\n",
    "\n",
    "---\n",
    "*Graph Structure Example:*  \n",
    "\n",
    "$ y = sin(e^{x}) + x$  \n",
    "Calculate $\\frac{dy}{dx}$ at x=5 \n",
    "\n",
    "Evaluation trace of forward mode of AD: \n",
    "\n",
    "| Trace | Elementary Function | Current Value |  Derivative          | Derivative Value |\n",
    "|-------|---------------------|---------------|----------------------|------------------|\n",
    "| $x_1$ |        $x_1$        |    5          | $\\dot{x_1}$          |        1         |\n",
    "| $x_2$ |        $e^{x_1}$    |    148.413    | $e^{x_1}\\dot{x_1}$   |        148.413   |\n",
    "| $x_3$ |        $sin(x_2)$   |    0.524      | $cos(x_2)\\dot{x_2}$  |       -126.425   |\n",
    "| $x_4$ |        $x_3 + x_1$  |    5.524      | $\\dot{x_3}+\\dot{x_1}$|       -125.425   |\n",
    "\n",
    "\n",
    "The computational graph can be seen below:  \n",
    "![example_graph](figs/milestone1_graph.png \"Computational Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future, we plan to make our package available on PyPI. However, under our current implementation, our package can be downloaded from our github repo at: https://github.com/ADdictedtoCS/cs207-FinalProject.git\n",
    "\n",
    "The user can store our package in a directory on their computer, and then, optionally, create a virtual environment before installing our package dependencies. A virtual enviroment would allow the user to compartmentalize the \n",
    "the dependencies for our package from the dependencies of other projects they might be working on.\n",
    "\n",
    "For users unfamiliar with virtual environments, the tool virtualenv can be easily installed via:\n",
    "\n",
    "    sudo easy_install virtualenv\n",
    "\n",
    "After downloading our package to a directory on their computer, the user can then activate a virtual environment and install dependencies via:\n",
    "\n",
    "    virtualenv env\n",
    "    source env/bin/activate\n",
    "    pip install -r requirements.txt\n",
    "    \n",
    "Later the user can deactivate the virtual enviroment via:\n",
    "\n",
    "    deactivate\n",
    "    \n",
    "Note that dependencies can also be installed directly without a virtual environment simply with:\n",
    "\n",
    "    pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can then import our package as follows...   \n",
    "*Note: These exact import statements assume the autodiff directory is in the user's current directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from autodiff.variable import Variable\n",
    "import autodiff.function as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for an example of how the user can interact with our package:\n",
    "\n",
    "1. Create a variable instantiated at an initial value \n",
    "    - Below we create variable x with a value of 0\n",
    "    - By default the gradient of this variable will be 1 (the seed value)\n",
    "2. Define a function \n",
    "    - Functions take variables as inputs and return new variables with updated value and gradient\n",
    "    - Below we define the function $sin(exp(x))$\n",
    "3. Call the function and get the value and gradient of the resulting variable \n",
    "    - The user can simply print the variable or alternatively call .val or .grad on the variable\n",
    "    - Value in this example: $sin(exp(0)) = 0.841$\n",
    "    - Gradient in this example: $exp(0)*cos(exp(0)) = 0.540$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x Value: [0.]\n",
      "Gradient: [1.]\n",
      "Output z Value: [0.84147098]\n",
      "Gradient: [0.54030231]\n",
      "The value is: [0.84147098]\n",
      "The gradient is: [0.54030231]\n"
     ]
    }
   ],
   "source": [
    "# Define a variable with an initial value\n",
    "x = Variable(0.)\n",
    "print(\"Input x\", x)\n",
    "\n",
    "# Define a function\n",
    "def my_func(x):\n",
    "    return F.sin(F.exp(x))\n",
    "\n",
    "# Variable z is the result of calling function on x\n",
    "z = my_func(x)\n",
    "\n",
    "# Get value and gradient of z\n",
    "print(\"Output z\", z)\n",
    "\n",
    "# Alternatively:\n",
    "print('The value is: {}'.format(z.val))\n",
    "print('The gradient is: {}'.format(z.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, users of our package may be interested in appyling it to solve optimization problems. As such, we provide an example of how our package could be used to implement the Newton-Raphson method for root finding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current x: [4.]\n",
      "Current x: [3.33333333]\n",
      "Current x: [2.88888889]\n",
      "Current x: [2.59259259]\n",
      "Current x: [2.39506173]\n",
      "Current x: [2.26337449]\n",
      "Current x: [2.17558299]\n",
      "Current x: [2.11705533]\n",
      "Current x: [2.07803688]\n",
      "Current x: [2.05202459]\n",
      "Current x: [2.03468306]\n",
      "Current x: [2.02312204]\n",
      "Current x: [2.01541469]\n",
      "Current x: [2.01027646]\n",
      "Current x: [2.00685097]\n",
      "Current x: [2.00456732]\n",
      "The root of the function is: [2.00456732]\n"
     ]
    }
   ],
   "source": [
    "def newtons_method(function, guess, epsilon):\n",
    "    x = Variable(guess)\n",
    "    f = function(x)\n",
    "    i = 0\n",
    "    max_out = False\n",
    "    while abs(f.val) >= epsilon and max_out == False:\n",
    "        x = x - f.val / f.grad\n",
    "        f = function(x)\n",
    "        print('Current x: {}'.format(x.val))\n",
    "        i += 1\n",
    "        if i >= 10000:\n",
    "            max_out = True\n",
    "    print('The root of the function is: {}'.format(x.val))\n",
    "            \n",
    "\n",
    "def my_func(x):\n",
    "    return 5*(x-2)**3\n",
    "\n",
    "guess = 5\n",
    "epsilon = 0.000001\n",
    "\n",
    "newtons_method(my_func, guess, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Directory Structure:\n",
    "\n",
    "We have our main implementation stored in the autodiff directory. This is where modules for our implementation of the forward mode of AD are located. The autodiff directory has a subdirectory containing tests.\n",
    "\n",
    "```\n",
    "cs207-FinalProject/\n",
    "    docs/\n",
    "        milestone1.ipynb\n",
    "        milestone1.ipynb\n",
    "    autodiff/\n",
    "        __init__.py\n",
    "        function.py\n",
    "        variable.py\n",
    "        utils.py\n",
    "        tests/\n",
    "            __init__.py\n",
    "            test_function.py\n",
    "            test_variable.py\n",
    "            test_utils.py\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    \n",
    "```\n",
    "##### Basic Modules:\n",
    "We included three modules in the autodiff directory as shown above. \n",
    "The function module contains our function class as well as the implementation of our elementary functions that constitute the computational graph. The variable module has our variable class as well as the implementation of all the basic operations (addition, multiplication, subtraction, division, power). This serves as the information flow within the computational graph. The utils module contains functions to be called by the variable class to make sure the input is of the correct type for the definition of a variable.\n",
    "\n",
    "In future implementations, we may end up breaking these modules out into separate modules for better readability. The aforementioned structure is also not exhaustive, and we may expand on it as we include future features.\n",
    "\n",
    "For example, we may also end up implementing the reverse mode of AD, in which case this would potentially be stored as a separate module within the autodiff directory.\n",
    "\n",
    "##### Testing:\n",
    "\n",
    "Our test suite lives within the autodiff directory (see above). Additionally for continuous integration and code coverage we utilize both TravisCI and CodeCov. We set up basic functionality for TravisCI and CodeCov for our repository.\n",
    "\n",
    "Tests can be run simply via:  \n",
    "    \n",
    "    !pytest\n",
    "\n",
    "##### Distribution and Packaging:\n",
    "\n",
    "In the future, we will distribute our package using PyPI (and upload with Twine). The user will be able to install our package via the command line: \"pip install autodiff\"\n",
    "\n",
    "However, for now the user can manually install our package by downloading our github repo from https://github.com/ADdictedtoCS/cs207-FinalProject.git , setting the current directory to the directory containing requirements.txt, and running pip install -r requirements.txt. Before doing this, the user may want to set up a virtual enviroment using virtualenv. For more details on precisely how the user can do this, along with examples of how to use our package, please refer to the *\"How to use autodiff\"* section above.\n",
    " \n",
    "##### Other Considerations:\n",
    "\n",
    "In the future, we may choose to build a GUI in order to make our package more accesible to end users with limited Python coding abilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "\n",
    "##### Core Data Structures:\n",
    "\n",
    "For each function defined by users, we create a Directed Acyclic Graph (DAG) data structure, which contains all computing nodes for the forward mode of AD.\n",
    "\n",
    "Our operation is based upon two main concepts: \n",
    "* **variable** carries the information flow through the computational graph. (see below).\n",
    "* **function** implements the elementary functions and constitutes the edges (or nodes, depending on ones's interpretation of the computational graph).\n",
    "\n",
    "Within each node, we compute the value and gradient of the function at that point in the computational graph. Therefore, the lists from all nodes form the computation table in the forward mode of AD.\n",
    "\n",
    "##### Classes:\n",
    "\n",
    "**Variable**  \n",
    "Our core class, which carries the information flow within the computational graph.\n",
    "\n",
    "Variables are initialized at a particular value passed in by the user, and have a default gradient of 1 (the \"seed vector\").\n",
    "\n",
    "All of the basic functionality for algebra involving variables is implemented within the Variable class through operator overloading.\n",
    "\n",
    "For example, we define what addition means for Variables in terms of their values and gradients here:\n",
    "\n",
    "```Python\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Variable):\n",
    "            out_val = self.val + other.val\n",
    "            out_grad = self.grad + other.grad\n",
    "            return Variable(val=out_val, grad=out_grad)\n",
    "        else:\n",
    "            new_val = get_right_shape(other)\n",
    "            out_val = self.val + get_right_shape(other)\n",
    "            out_grad = self.grad\n",
    "            return Variable(val=out_val, grad=out_grad)\n",
    "```\n",
    "\n",
    "In the current implementation, Variable only holds a single value and gradient value (a single derivative). However, in the near future we plan to expand this functionality to allow for Variable to hold an array of values and an array for gradient as well. For this functionality, we will rely on numpy arrays.\n",
    "\n",
    "\n",
    "**Function**  \n",
    "When called on a variable, a function returns a new variable with a transformed value and gradient.\n",
    "\n",
    "A function has the methods *get_val* and *get_grad* that denote how the function transforms a variables val and grad. Note that these methods are not implemented for the base class but for the elementary functions which are subclasses of function (see below for more details).\n",
    "\n",
    "The function class is also where we implement the chain rule for calculating gradients:\n",
    "\n",
    "```Python\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Implements the chain rule.\n",
    "        Input: autodiff.Variable type holding a val and grad\n",
    "        Output:  autodiff.Variable type holding the val, grad of the\n",
    "        transformed variable\n",
    "        \"\"\"\n",
    "        out_val = self.get_val(x.val)\n",
    "        out_grad = np.dot(self.get_grad(x.val), x.grad)\n",
    "        return Variable(val=out_val, grad=out_grad)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "##### Important Attributes:\n",
    "\n",
    "In our current implementation, the key attributes for the **Variable** class are:\n",
    "\n",
    "*val* = The value. Can be an int or float. When a variable is first initialized, this is passed in by the user.\n",
    "\n",
    "*grad* = The gradient. Can be an int or float. When the variable is first initialized this has a default value of 1 (the \"seed vector\").\n",
    "\n",
    "As mentioned above, functions take in a variable and output a new variable with a transformed value and gradient.\n",
    "\n",
    "The value and gradient of a variable can be obtained by the user by calling .val or .grad on the variable. Additionally, the user can print a variable to see its value and gradient.\n",
    "\n",
    "```Python\n",
    "    # Define a variable with an initial value\n",
    "    x = Variable(5.)\n",
    "    \n",
    "    x.val\n",
    "    >>>> [5.]\n",
    "    \n",
    "    x.grad\n",
    "    >>>> [1.]\n",
    "    \n",
    "    print(x)\n",
    "    >>>> Value: [5.]\n",
    "    >>>> Gradient: [1.]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### External dependencies:\n",
    "External dependencies for our package are listed in requirements.txt\n",
    "\n",
    "For now the only required external dependency is Numpy.\n",
    "\n",
    "Required:\n",
    "- Numpy 1.16.4 (for vector operations)\n",
    "\n",
    "\n",
    "#### Elementary functions:\n",
    "\n",
    "The elementary functions such as **sin**, **exp**, etc are all implemented as subclasses of **Function**.\n",
    "\n",
    "For each elementary function, we explicitedly define its value and derivative. For example, $e^x$ below:\n",
    "\n",
    "```Python\n",
    "    class Exponent(Function):\n",
    "        \"\"\"Exponential\"\"\"    \n",
    "        def get_val(self, x):\n",
    "            return np.exp(x)\n",
    "    \n",
    "        def get_grad(self, x):\n",
    "            return np.exp(x)\n",
    "```\n",
    "\n",
    "In our current implementation, the elementary functions we have implemented along with their derivatives are as follows:\n",
    "\n",
    "| Elementary Function |  Derivative            |\n",
    "|---------------------|------------------------|\n",
    "|        $e^x$        |   $e^x$                |\n",
    "|        $sin(x)$     |   $cos(x)$             |\n",
    "|        $cos(x)$     |   $-sin(x)$            |\n",
    "|        $tan(x)$     |   $\\frac{1}{cos(x)^2}$ | \n",
    "\n",
    "In the near future we hope to add additional elementary functions such as $arcsin(x)$ to our package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Features\n",
    "\n",
    "##### Vector Valued Functions:\n",
    "\n",
    "Our immediate next step is to update our package to allow for functions with multiple inputs and outputs. We can accomplish this by reworking our variable class to allow for an array of inputs and to compute the full gradient at each step of the computational graph.\n",
    "\n",
    "Thus, we will also have to update our code to incorporate matrix algebra, for which we plan to rely on the numpy package.\n",
    "\n",
    "The changes will be in the following aspects:\n",
    "\n",
    "* The initialization of Variable objects will also accept vector based values, which could be relied on the numpy lib.\n",
    "* The attributes of Variable class, \"val\" and \"grad\", will be vector values as well.\n",
    "* Operators like \"+\" \"-\" \"\\*\" \"/\" should be redefined. Each operator should focus on vector operations. It is easy to do it for \"+\" \"-\" \"\\*\", but for \"/\" and \"\\*\\*\", vector values will not work sometimes. \n",
    "* For \"\\*\" operator, a lot more work should be done. This operator should accept matrix values, and check whether two matrices (vectors) are able to perform \"\\*\" operation. The definition of gradients should also change in this case.\n",
    "* For elementary functions, they should check whether they accept vector variables as inputs.\n",
    "\n",
    "##### Multiple Variable Functions:\n",
    "\n",
    "Different from vector valued function, a function could accept multiple variables as inputs. Sometimes it is hard to aggregate all variables into one vector of variable, then supporting multiple variables could be easier. \n",
    "\n",
    "The changes will be in the following aspects:\n",
    "\n",
    "* The definition of \"grad\" should change. In the forward mode of autodiff, a variable should record the gradient for each \"root variable\" (variables we create manually). Therefore, we can change \"grad\" as a **dict** of gradients, with one gradient value for each root variable. \n",
    "* The definition of operators should change. When creating a new variable when performing an operation between two variables, we should calculate the gradient for each root variable. If the gradient of one root variable does not exist for one variable, then it is 0 for that variable. Same changes should be performed on elementary functions.\n",
    "\n",
    "##### Argument Parser:\n",
    "\n",
    "An additional feature we would like to implement that we believe could improve ease of use of our package for users is an argument parser, which would allow users to input functions as strings.\n",
    "\n",
    "In our current implementation, as shown in the \"How to use autodiff\" section, the user must enter functions by specifically calling elementary functions such as:\n",
    "\n",
    "    F.sin(F.exp(x))\n",
    "    \n",
    "Our argument parser would be built so that users could have the option of entering functions more naturally as strings such as:\n",
    "\n",
    "    \"sin(e^x)\"\n",
    "   \n",
    "We would use regular expressions to implement our argument parser in a way that is robust and accounts for a wide range of possible user inputs.\n",
    "\n",
    "##### Reverse Mode:\n",
    "\n",
    "As mentioned in the \"Software Organization\" section above, we may want to implement the reverse mode of autodifferentiation in the future. This could be a useful addition to our package, because, as mentioned above the reverse mode is more efficient when the number of independent variables is much greater than the number of functions. We would likely create reverse mode as a separate module within our autodiff directory.\n",
    "\n",
    "Several changes should be performed:\n",
    "\n",
    "* Because reverse mode includes a forward pass and a reverse calculation, we may record the graph structrure when we create the final function. In other words, when we create a new variable using a function (or variable operators) from 1 or 2 base variables, the function should record those base variables, so that we can do the reverse mode.\n",
    "* The definition of gradient for Variable class should be optimized. Because we only need to record at most 2 gradients (one for each child base variable), we can use a tuple or just use two attributes to record it within the Variable class.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
