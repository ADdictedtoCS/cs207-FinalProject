{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More for bckground? What was wrong latex didnt render in git?\n",
    "\n",
    "utils? Where does that go. what is it? moved variables\n",
    "\n",
    "Core data structures and classes section. Need more info. What is is in init statement. What properties do each class have)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "## CS207 Final Project, Group 28\n",
    "#### Team Members:\n",
    "* Josh Bodner  \n",
    "* Théo Guenais  \n",
    "* Daiki Ina  \n",
    "* Junzhi Gong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ever since the notion of a derivative was first defined in the days of Newton and Leibniz, differentiation has become a crucial component across quantitative analyses. Today, differentiation is applied within a wide range of scientific disciplines from cell biology to electrical engineering and astrophysics. Differentiation is also central to the domain of optimization where it is applied to find maximum and minimum values of functions.\n",
    "\n",
    "Because of the widespread use and applicability of differentiation, it would be highly beneficial if scientists could efficiently calculate derivatives of functions using a Python package. Such a package could save scientists time and energy from having to compute derivatives symbolically, which can be especially complicated if the function of interest has vector inputs and outputs.\n",
    "\n",
    "Our Python package, autodiff, will address this need by allowing the user to implement the forward mode (and potentially the reverse mode) of automatic differentiation (AD). Using AD, we are able to calculate derivatives to machine precision in a manner that is less costly than symbolic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In this section we provide a brief overview of the mathematical concepts relevant to our implementation:\n",
    "\n",
    "##### 1. Multivariable Differental Calculus:\n",
    "\n",
    "Let $f: \\mathbb{R^m} \\rightarrow \\mathbb{R^n}: x \\mapsto f(x)$. Under certain regularity and smoothness assumptions, we define the derivative of f as $f': \\mathbb{R^m} \\rightarrow L_{\\mathbb{R^m}, \\mathbb{R^n}}$, with $L_{\\mathbb{R^m}, \\mathbb{R^n}}$ the space of linear mapping $\\mathbb{R^m}\\rightarrow \\mathbb{R^n}$ which can be transformed to $\\mathbb{R^{mxn}}$.\n",
    "\n",
    "*This general definition might be broken due to lack of smoothness, in which case, we refer to \"directional derivatives\" and specifically \"Gateaux-derivatives\".*\n",
    "\n",
    "From this lense, we better understand the idea that the derivative of a function evaluated at a point can be associated to a matrix and when this function is real-valued, we obtain a vector $\\nabla_x f \\in \\mathbb{R^m}$.\n",
    "\n",
    "Specifically, the gradient of a scalar-valued multivariable function is a vector of its partial derivatives with respect to each input:\n",
    "\n",
    "$\\nabla f = \n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_3} \\\\\n",
    "    \\vdots\n",
    "  \\end{bmatrix}$\n",
    "\n",
    "\n",
    "##### 2. Chain Rule\n",
    "Let $f: \\mathbb{R^m} \\rightarrow \\mathbb{R^n}$ and $g: \\mathbb{R^p} \\rightarrow \\mathbb{R^m}$. \n",
    "Then $f \\circ g: \\mathbb{R^p} \\rightarrow \\mathbb{R^n}$ is such that (under regularity assumptions), $(f \\circ g)'(x) = f'(g(x)) \\cdot g'(x)$. This operation rule, usually known as the \"chain rule\" has become increasingly popular for the last decades due to better computational algorithms and availibility of computational power (e.g BackProp algorithm). \n",
    "\n",
    "##### 3. Graph Structure and elementary function\n",
    "Therefore, the space of functions whose gradients an values are tractable is stable under the composition operator and the chain rule can be applied N-times in order to evaluate the gradient of a function (composed of N sub-functions) with respect to some parameters, at a given evaluation point. \n",
    "\n",
    "Furthermore, we add that some theoretical results advance that functions of general form (e.g bounded function), that can be approximated by elementary functions, hence the increased interest in the functions that are simple compositon of elementary functions and whose derivatives can be evaluated to machine precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user will be able to import our package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autodiff as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are considering 2 possibilities for how the user will interact with our package.\n",
    "\n",
    "We will be working out which option makes more sense as we begin the process of development, however, option 1 seems like the natural first step whereas the second option would likely be a follow-up implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option 1 ---\n",
    "\n",
    "# The user could instantiate a variable at a particular evaluation point.\n",
    "# Next, a function could be expressed as a composition of the elementary functions..\n",
    "\n",
    "x = AD.variable(5) # Evaluate at point x = 5\n",
    "y = AD.function.exp(x)\n",
    "y.gradient\n",
    ">>> 148.413159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option 2 ---\n",
    "\n",
    "# The user could instantiate autodiff variables and create functions by\n",
    "# performing operations on those autodiff variables.\n",
    "# They could then call get_der to get the derivative.\n",
    "\n",
    "x = AD.variable(length of input vector)\n",
    "y = AD.variable(length of input vector)\n",
    "\n",
    "my_function = 4 * x + sqrt(x) + x * y\n",
    "\n",
    "function.get_derivative(target=x, x=2, y=3)\n",
    ">>> 7.3535533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Directory Structure:\n",
    "\n",
    "We will have our main implementation stored in the autodiff directory. This will have a subdirectory for our implementation of forward mode, which in turn will have a subdirectory containing tests.\n",
    "\n",
    "```\n",
    "cs207-FinalProject/\n",
    "    docs/\n",
    "        milestone1.ipynb\n",
    "        ...\n",
    "    autodiff/\n",
    "        __init__.py\n",
    "        functional/\n",
    "            __init__.py\n",
    "            function.py\n",
    "            variable.py\n",
    "            utils.py\n",
    "        tests/\n",
    "            __init__.py\n",
    "            test_function.py\n",
    "            test_variable.py\n",
    "            test_utils.py\n",
    "            ...\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    \n",
    "```\n",
    "##### Modules to Include:\n",
    "We will include a functional module containing our main functionalities and especially the implementation of our elementary functions that constitute our computational grpah. We may end up breaking this out into further modules for better readability. Furthermore, the aforementioned structure is not exhaustive.\n",
    "\n",
    "We may also end up implementing the reverse mode, in which case this would potentially be stored as a separate module within the autodiff module.\n",
    "\n",
    "##### Testing:\n",
    "\n",
    "Our test suite will live within our autodiff directory (see above). Additionally for continuous integration and code coverage we will be utilizing both TravisCI and CodeCov. We have already set up basic functionality for TravisCI and CodeCov for our repository. Related documentation will also be available to the user.\n",
    "\n",
    "##### Distribution and Packaging:\n",
    "\n",
    "We will distribute our package using PyPI (upload with Twine).\n",
    "The user can then install our package via the command line: \"pip install autodiff\"\n",
    " \n",
    "##### Other Considerations:\n",
    "\n",
    "We may choose to build a GUI in order to make our package more accesible to end users with limited Python coding abilities. The possibility of building a static graph in order to repeat and speed up the evaluation of a given derivative at several points is also an option we are considering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "##### Core Data Structures:\n",
    "\n",
    "For each function defined by users, we create a Directed Acyclic Graph (DAG) data structure for it, which contains all computing nodes for the forward mode AD (and potentially the reverse mode AD).\n",
    "\n",
    "Our operation swill be based upon two main concepts: \n",
    "* **AD.variable** carries the information flow through the computational graph. (see below).\n",
    "* **AD.function** implements the elementary functions and constitutes the edges (or nodes, depending on ones's interpretation of the computational graph).\n",
    "\n",
    "Within each node, there is a list of values and derivatives, which could be used to complete the forward mode (and reverse mode) AD computation. Therefore, the lists from all nodes form the computation table in either forward mode AD and reverse mode AD. We could also include some other metadata along with those variables.\n",
    "\n",
    "##### Classes:\n",
    "\n",
    "The classes we will implement include the **Function** (possibly both a forward and reverse mode version), and **Variable**. Additionally, we may implement the elementary functions as subclasses of **Function**.\n",
    "\n",
    "##### Methods and name attributes:\n",
    "\n",
    "Users could start by creating **AD.variable**s, which could be either a scalar value or a vector of values. Users could call **x = AD.variable(init)** to create a variable, for any multidimensional vector **init**.\n",
    "\n",
    "Users could then create a function by applying operations on **AD.variable**s. The basic operations includes \"+\" \"-\" \"\\*\" \"/\", and other elementary functions, including **AD.function.exp()**, **AD.function.sin()**, **AD.function.log()**, *e.t.c.* For example, a user can create a function in the following way:\n",
    "\n",
    "```Python\n",
    "def my_func(x):\n",
    "    x1 = AD.function.sin(x)\n",
    "    x2 = AD.function.exp(x1)\n",
    "    return AD.function.cos(x1) + x2\n",
    " \n",
    "x = AD.variable(1000)\n",
    "y = my_func(x)\n",
    "y.grad\n",
    "```\n",
    "\n",
    "**Function** will have the methods **get_val()** and **get_der()** to get its value and derivative.\n",
    "\n",
    "The **AD.function**  could also include the following attributes, but none of them is necessary for the first version of the code and the rudimentary implementation.\n",
    "\n",
    "- val (Current value)-in that case, we would have to make sure that a function points to a Variable.\n",
    "- der (Current derivative). Idem.\n",
    "- children / graph (Record the graph structure)\n",
    "- val_list (Value table for forward mode and reverse mode)\n",
    "- der_list (Derivative table for forward mode and reverse mode)\n",
    "- mode (forward or reverse)\n",
    "\n",
    "\n",
    "#### External dependencies:\n",
    "Required:\n",
    "- Numpy (for vector operations)\n",
    "\n",
    "Additional possibilities:  \n",
    "- Math (for scientific math functions)\n",
    "- Matplotlib (for graphing if we end up building a GUI)\n",
    "\n",
    "#### Elementary functions:\n",
    "\n",
    "We will the elementary functions such as **sin**, **sqrt**, **log**, **exp**, etc. as *subclasses* **Function**. \n",
    "Those classes have the same feature as common **AD.function**s, but we have determine the internal functionalities, including how to calculate the derivatives.\n",
    "\n",
    "#### Components that must be taken into account for implementation:\n",
    "\n",
    "There are two components, concepts of \"Variable\" and \"Function\", to achieve information flow:\n",
    "\n",
    "1. Concept of “Variable” \n",
    "    - The variables carry the information flow, which will be used when performing computation on the graph in the forward mode or the reverse mode of AD. The variable would have ```val``` (value) and ```grad``` (gradient) attributes, consistently with the trace of the forward mode operations.\n",
    "    \n",
    "2. Concept of \"Function\"\n",
    "    - How to make the information flow using \"Variable\"? Function is used for this purpose. The Function implements the atomic operations of the graph. It would take as input a ```AD.variable``` and outputs an object of the same type, with the updated ```val``` and ```grad``` attributes. \n",
    "    \n",
    "Below is pseudocode to show how a function brings information flow using a variable. The code is rudimentary and does not take into account the parent class, static methods,... This demonstration is for illustrative purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autodiff as AD\n",
    "\n",
    "x = AD.Variable(0)\n",
    "y = AD.function.exp(x)\n",
    "\n",
    "y.grad\n",
    ">>> 1\n",
    "\n",
    ">>> y.val\n",
    ">>> 1 \n",
    "\n",
    "#Internally\n",
    "class exp():\n",
    "    def get_val(self, x):\n",
    "        return numpy.exp(x)\n",
    "    \n",
    "    def get_der(self, x):\n",
    "        return numpy.exp(x)\n",
    "    \n",
    "    def __call__(self, x:AD.variable)->AD.variable\n",
    "        output = AD.variable(self.get_val(x.val) #Gradient initialize at np.ones(len(input))\n",
    "        output.grad = self.get_derivative(x.val) * x.grad #Chain rule\n",
    "        return output \n",
    "                             \n",
    "\n",
    "# Another idea that could simplify all of the operations:\n",
    "    #Have the chain rule implemented for all the entire class (@static method).\n",
    "    #Question we should aks ourselves: inplace operations ? garbage collection ? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
