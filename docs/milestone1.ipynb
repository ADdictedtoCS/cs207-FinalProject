{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "## CS207 Final Project, Group 28\n",
    "#### Team Members:\n",
    "Josh Bodner  \n",
    "Theo Guenais  \n",
    "Daiki Ina  \n",
    "Junzhi Gong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ever since the notion of a derivative was first defined in the days of Newton and Leibniz, differentiation has become a crucial component across quantitative analyses. Today, differentiation is applied within a wide range of scientific disciplines from cell biology to electrical engineering and astrophysics. Differentiation is also central to the domain of optimization where it is applied to find maximum and minimum values of functions.\n",
    "\n",
    "Because of the widespread use and applicability of differentiation, it would be highly beneficial if scientists could efficiently calculate derivatives of functions using a Python package. Such a package could save scientists time and energy from having to compute derivatives symbolically, which can be especially complicated if the function of interest has vector inputs and outputs.\n",
    "\n",
    "Our Python package, autodiff, will address this need by allowing the user to implement both the forward and reverse modes of automatic differentiation (AD). Using AD, we are able to calculate derivatives to machine precision in a manner that is less costly than symbolic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In this section we provide a brief overview of the mathematical concepts relevant to our implementation:\n",
    "\n",
    "##### 1. Multivariable Functions:\n",
    "\n",
    "A multivariable function $f(x)$ can have m inputs and n outputs. These inputs and outputs are vectors. From this lense, f(x) can be understood as mapping from $\\mathbb{R^m}$ to $\\mathbb{R^n}$. In other words $f(x): \\mathbb{R}^{m} \\mapsto \\mathbb{R}^{n}$  \n",
    "\n",
    "*Example:*  \n",
    "$f(x_1, x_2) = \n",
    "  \\begin{bmatrix}\n",
    "    sin(x_1)x_2^2\n",
    "  \\end{bmatrix}$\n",
    "\n",
    "Here we have...  \n",
    "$x = \n",
    "  \\begin{bmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2\n",
    "  \\end{bmatrix} \\in\\mathbb{R}^{2}$  \n",
    "    \n",
    "$f(x_1, x_2) \\in\\mathbb{R}^{1}$\n",
    "\n",
    "##### 2. Gradient:\n",
    "\n",
    "The gradient of a scalar-valued multivariable function is a vector of its partial derivatives with respect to each input:\n",
    "\n",
    "$\\nabla f = \n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_3} \\\\\n",
    "    \\vdots\n",
    "  \\end{bmatrix}$\n",
    "\n",
    "\n",
    "##### 3. Chain Rule\n",
    "*FILL IN*\n",
    "\n",
    "##### 4. Graph Structure\n",
    "*FILL IN*\n",
    "\n",
    "##### 5. Elementary Functions\n",
    "*FILL IN*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use autodif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user could import our package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autodiff as AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have two ideas of how the user might go about using our package.\n",
    "\n",
    "We will be working out which option makes more sense as we begin the process of development..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Option 1 ---\n",
    "\n",
    "# The user could instantiate a function by typing out the expression\n",
    "# of that function. They could then call get_der to get the derivative.\n",
    "\n",
    "function = AD.ADFunction(“4 * x + sqrt(x) + x * y”)\n",
    "function.get_der(target='x', x=2, y=3)\n",
    ">>> 7.3535533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-853be072e192>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-853be072e192>\"\u001b[1;36m, line \u001b[1;32m7\u001b[0m\n\u001b[1;33m    x = AD.ADVariable(length of input vector)\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# --- Option 2 ---\n",
    "\n",
    "# The user could instantiate autodiff variables and create functions by\n",
    "# performing operations on those autodiff variables.\n",
    "# They could then call get_der to get the derivative.\n",
    "\n",
    "x = AD.ADVariable(length of input vector)\n",
    "y = AD.ADVariable(length of input vector)\n",
    "\n",
    "function = 4 * x + sqrt(x) + x * y\n",
    "\n",
    "function.get_der(target=x, x=2, y=3)\n",
    ">>> 7.3535533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Directory Structure:\n",
    "\n",
    "We will have our main implementation stored in the autodiff directory. This will have a subdirectory for our implementation of forward mode, which in turn will have a subdirectory containing tests.\n",
    "\n",
    "```\n",
    "cs207-FinalProject/\n",
    "    docs/\n",
    "        milestone1.ipynb\n",
    "    autodiff/\n",
    "        ForwardMode/\n",
    "            __init__.py\n",
    "            forwardmode.py\n",
    "            Tests/\n",
    "                __init__.py\n",
    "                test_forwardmode.py\n",
    "    README.md\n",
    "    Requirements.txt\n",
    "    \n",
    "```\n",
    "##### Modules to Include:\n",
    "We will include a forwardmode module containing our basic functionality. We may end up breaking this out into separate modules for better readability, including an adfunctions module and an advariables module.\n",
    "\n",
    "We may also end up implementing the reverse mode, in which case this would also be stored as a separate module within a directory structure matching that of ForwardMode above.\n",
    "\n",
    "##### Testing:\n",
    "\n",
    "Our test suite will live within our ForwardMode directory (see above). Additionally for continuous integration and code coverage we will be utilizing both TravisCI and CodeCov. We have already set up basic functionality for TravisCI and CodeCov for our repository.\n",
    "\n",
    "##### Distribution and Packaging:\n",
    "\n",
    "We will distribute our package using PyPI. We will use Twine to upload our distributions to PyPI.\n",
    "\n",
    "The user can then install our package via a pip install such as:\n",
    "\"pip install autodiff\"\n",
    " \n",
    "##### Other Considerations:\n",
    "\n",
    "We may choose to build a GUI in order to make our package more accesible to end users with limited Python coding abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "##### Core Data Structures:\n",
    "\n",
    "We will have a DAG data structure for ADfunctions. Each DAG node will have a list of values and derivatives, possibly along with some other metadata. \n",
    "\n",
    "##### Classes:\n",
    "\n",
    "The classes we will implement include ADFunction (possibly both a forward and reverse mode version), ADVariable, and possibly some other subclasses of ADFunction such as elementary functions.\n",
    "\n",
    "##### Methods and name attributes:\n",
    "\n",
    "ADFunction will have the methods get_val() and get_der() to get its value and derivative. \n",
    "\n",
    "\n",
    "* FILL IN MORE *\n",
    "\n",
    "##### External dependencies:\n",
    "Required:\n",
    "- Numpy (for vector operations)\n",
    "\n",
    "Additional possibilities:  \n",
    "- Math (for scientific math functions)\n",
    "- Matplotlib (for graphing if we end up building a GUI)\n",
    "\n",
    "##### Elementary functions:\n",
    "\n",
    "We will likely implement the elementary functions such as sin, sqrt, log, exp, etc. as subclasses of ADFunction. Each would then have two major methods: get_val() and get_der() to calculate their values and derivatives.\n",
    "\n",
    "##### Components that must be taken into account for implementation:\n",
    "There are two components, concepts of \"Variable\" and function, to achieve information flow \n",
    "1. Concepts of “Variable” \n",
    "    - The variable (which could be vectors) carries the information flow (cf the old PyTorch implementation and idea of computational graph). The variable would have value and gradients attribute (x.val and x.grad) that would carry the information for the computational graph.\n",
    "    \n",
    "\n",
    "2. Idea of function\n",
    "    - How to make the information flow using \"Variable\" ? Function is used for this purpose. We feed function a variable and it returns another variable. If x is a variable with x.value and x.gradient attributes: let y = exp(x) or maybe something less trivial, i.e y = AD.fn.tan(x). The functional class would allow us to break any function into computational units. A function has two main methods, AD.fn.get_grad() and AD.fn.get_val(). The get_grad would actually implement the chain rule. \n",
    "\n",
    "“Functional component”-write our own implementation of all the elementary functions.\n",
    "Below is pseudocode to show how a function brings information flow using a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autodiff as AD\n",
    "\n",
    "x = AD.Variable(0)\n",
    "y = AD.FN.exp(x)\n",
    "\n",
    "# Internally \t\n",
    "    y=Variable() \n",
    "    y.val = fn.exp.get_val(x.val) \n",
    "    y.grad = fn.exp._derivative_fn(x.val) * x.grad #implement the chain rule. \n",
    "\n",
    "# Another idea\n",
    "\tHave the chain rule implemented for the entire class (class methods) \n",
    "\n",
    "# Class fn\n",
    "\tDef (self, input_variable):\n",
    "\n",
    "Output = Variable()\n",
    "Output.val = self.eval(x.val)\n",
    "Output.grad = self.grad_fn(x.val) * x.grad #0r np.dot(..) for multidimensional arrays and so on \n",
    "And then all the elementary functions would just have fn.exp.eval, and fn.exp.grad_fn \n",
    "YES: IMPLEMENT THE CHAIN RULE AS A BASE CLASS METHOD. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
