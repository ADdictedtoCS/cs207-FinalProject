{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "## CS207 Final Project, Group 28\n",
    "#### Team Members:\n",
    "* Josh Bodner  \n",
    "* ThÃ©o Guenais  \n",
    "* Daiki Ina  \n",
    "* Junzhi Gong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Ever since the notion of a derivative was first defined in the days of Newton and Leibniz, differentiation has become a crucial component across quantitative analyses. Today, differentiation is applied within a wide range of scientific disciplines from cell biology to electrical engineering and astrophysics. Differentiation is also central to the domain of optimization where it is applied to find maximum and minimum values of functions.\n",
    "\n",
    "Because of the widespread use and applicability of differentiation, it would be highly beneficial if scientists could efficiently calculate derivatives of functions using a Python package. Such a package could save scientists time and energy from having to compute derivatives symbolically, which can be especially complicated if the function of interest has vector inputs and outputs.\n",
    "\n",
    "Our Python package, **autodiff**, addresses this need by allowing the user to implement the forward mode of automatic differentiation (AD). Using AD, we are able to calculate derivatives to machine precision in a manner that is less costly than symbolic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "In this section we provide a brief overview of the mathematical concepts relevant to our implementation:\n",
    "\n",
    "##### 1. Multivariable Differental Calculus:\n",
    "\n",
    "Let $f: {\\rm R^m} \\rightarrow {\\rm R^n}: x \\mapsto f(x)$ . Under certain regularity and smoothness assumptions, we define the derivative of f as $ f': {\\rm R^m} \\rightarrow L_{{\\rm R^m}, {\\rm R^n}}$, with $ L_{{\\rm R^m}, {\\rm R^n}}$ being the space of the linear mapping $ {\\rm R^m}\\rightarrow {\\rm R^n}$ which can be transformed to $ {\\rm R^{mxn}}$.\n",
    "\n",
    "*This general definition might be broken due to lack of smoothness, in which case, we refer to \"directional derivatives\" and specifically \"Gateaux-derivatives\".*\n",
    "\n",
    "From this lense, we can understand the derivative of a function evaluated at a given point to be a matrix. When this function is real-valued, we can obtain a vector $ \\nabla_x f \\in {\\rm R^m} $.\n",
    "\n",
    "Specifically, the gradient of a scalar-valued multivariable function is a vector of its partial derivatives with respect to each input:\n",
    "\n",
    "$\\nabla f = \n",
    "  \\begin{bmatrix}\n",
    "    \\frac{\\partial f}{\\partial x_1} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "    \\frac{\\partial f}{\\partial x_3} \\\\\n",
    "    \\vdots\n",
    "  \\end{bmatrix}$\n",
    "\n",
    "##### 2. Chain Rule\n",
    "Let $ f: {\\rm R^m} \\rightarrow {\\rm R^n} $ and $ g: {\\rm R^p} \\rightarrow {\\rm R^m} $. \n",
    "Then $ f \\circ g: {\\rm R^p} \\rightarrow {\\rm R^n} $ is such that (under regularity assumptions), $ (f \\circ g)'(x) = f'(g(x)) \\cdot g'(x) $. This operational rule, known as the chain rule, is a crucial part of AD.\n",
    "\n",
    "---\n",
    "\n",
    "*Example:*  \n",
    "$ f(x) = sin(x)$  \n",
    "$ g(x) = e^{x}$  \n",
    "$ (f \\circ g)(x) = sin(e^{x})$  \n",
    " \n",
    "$(f \\circ g)'(x) = f'(g(x))g'(x) = cos(e^{x})e^{x}$\n",
    "\n",
    "##### 3. Automatic Differentiation\n",
    "\n",
    "*Automatic Differentiation and Forward Mode:* \n",
    "\n",
    "Automatic differentiation is a process for evaluating derivatives computationally at a particular evaluation point. Specifically, the forward mode of automatic differentiation calculates the product of the gradient with a seed vector $ \\nabla f \\cdot p $ or the product of the Jacobian with a seed vector ($Jp$) if $f$ is a multi-dimensional function.\n",
    "\n",
    "In the forward mode of automatic differentiation, the calculation is done in steps where the partial derivatives and values are computed at each step of the computational graph. The edges of the computational graph are such that transitioning from node to node involves simple operations or calculation of elementary functions such as $sin(x)$, $cos(x)$, $e^{x}$, etc.\n",
    "\n",
    "*Precision and Efficiency:*\n",
    "\n",
    "Automatic differentiation is able to evaluate derivatives to machine precision. This is because it computes exact derivatives using the component elementary functions at each step of the computational graph. The precision of automatic differentiation is one of its advantages over other methods such as numerical differentiation, which can suffer from floating point precision errors.\n",
    "\n",
    "The forward mode of automatic differentiation is also particularly efficient when the the number of functions to evaluate is much larger than the number of independent variables $ n >> m$  for $f: {\\rm R^m} \\rightarrow {\\rm R^n}$. This is because forward mode involves performing one sweep over the computational graph for each of the m independent variables. In comparison, the reverse mode of automatic differentiation requires n sweeps over the computational graph and is thus more efficient when $m >> n$.\n",
    "\n",
    "---\n",
    "*Graph Structure Example:*  \n",
    "\n",
    "$ y = sin(e^{x}) + x$  \n",
    "Calculate $\\frac{dy}{dx}$ at x=5 \n",
    "\n",
    "Evaluation trace of forward mode of AD: \n",
    "\n",
    "| Trace | Elementary Function | Current Value |  Derivative          | Derivative Value |\n",
    "|-------|---------------------|---------------|----------------------|------------------|\n",
    "| $x_1$ |        $x_1$        |    5          | $\\dot{x_1}$          |        1         |\n",
    "| $x_2$ |        $e^{x_1}$    |    148.413    | $e^{x_1}\\dot{x_1}$   |        148.413   |\n",
    "| $x_3$ |        $sin(x_2)$   |    0.524      | $cos(x_2)\\dot{x_2}$  |       -126.425   |\n",
    "| $x_4$ |        $x_3 + x_1$  |    5.524      | $\\dot{x_3}+\\dot{x_1}$|       -125.425   |\n",
    "\n",
    "\n",
    "The computational graph can be seen below:  \n",
    "![example_graph](figs/milestone1_graph.png \"Computational Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install autodiff package:**\n",
    "\n",
    "----\n",
    "*Option 1:*\n",
    "\n",
    "Our package is available on PyPI. The preferred way for installing our package via PyPI is as follows:\n",
    "\n",
    "First, the user can (optionally) create a virtual environment before installing our package dependencies. A virtual enviroment would allow the user to compartmentalize the the dependencies for our package from the dependencies of other projects they might be working on.\n",
    "\n",
    "For users unfamiliar with virtual environments, the tool virtualenv can be easily installed via:\n",
    "\n",
    "    sudo easy_install virtualenv\n",
    "\n",
    "The user can then activate a virtual environment via:\n",
    "\n",
    "    virtualenv env\n",
    "    source env/bin/activate\n",
    "    \n",
    "Note: the user can later deactivate the virtual enviroment via:\n",
    "\n",
    "    deactivate\n",
    "    \n",
    "Then, the user can simply install our package from PyPI via:\n",
    "\n",
    "    pip install autodiff-ADdictedtoCS\n",
    "\n",
    "----\n",
    "*Option 2:*\n",
    "\n",
    "Alternatively, the autodiff package can be downloaded directly from our organization's github repository at: https://github.com/ADdictedtoCS/cs207-FinalProject.git\n",
    "\n",
    "The user can then install the requirements via:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "    \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user can then import our package as follows...   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import statements\n",
    "import autodiff\n",
    "import autodiff.function as F\n",
    "import autodiff.optim as optim\n",
    "from autodiff.variable import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for an example of how the user can interact with our package:\n",
    "\n",
    "1. Create a \"variable\" instantiated with a vector of initial values\n",
    "    - For a multivariate function of 3 inputs for example, \"variable\" takes in the initial value\n",
    "      of each input\n",
    "2. Define a function \n",
    "    - Functions take variables as inputs and return new variables with updated values and gradients\n",
    "3. Call the function and get the value and gradient of the resulting variable \n",
    "    - The user can simply print the variable or alternatively call .val or .grad on the variable\n",
    "\n",
    "*Remarks*: \n",
    "- The user can benefit from a natural and simple way to interact with the Variables and Functions.\n",
    "-  autodiff.function also supports a range of different elementary functions. See \"Implementation Details\" section below for further details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Single input, single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      "Value: 0.0\n",
      "Gradient: 1.0\n",
      "\n",
      "Output z:\n",
      "Value: 0.8414709848078965\n",
      "Gradient: 0.5403023058681398\n",
      "The value is: 0.8414709848078965\n",
      "The gradient is: 0.5403023058681398\n"
     ]
    }
   ],
   "source": [
    "# Example R^1 -> R^1\n",
    "\n",
    "# Define a variable with an initial value\n",
    "x = Variable(0.)\n",
    "print(\"Input x:\")\n",
    "print(x)\n",
    "\n",
    "# Define a function\n",
    "def my_func(x):\n",
    "    return F.sin(F.exp(x))\n",
    "\n",
    "# Variable z is the result of calling function on x\n",
    "z = my_func(x)\n",
    "\n",
    "# Get value and gradient of z\n",
    "print(\"\\nOutput z:\")\n",
    "print(z)\n",
    "\n",
    "# Alternatively:\n",
    "print('The value is: {}'.format(z.val))\n",
    "print('The gradient is: {}'.format(z.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we initialize a variable x with a value of 0\n",
    "\n",
    "We then define the function:\n",
    "ð‘“(ð‘¥)=ð‘ ð‘–ð‘›(ð‘’ð‘¥ð‘(ð‘¥)) \n",
    "Lastly, we get the value and the gradient of the function at the initial point:\n",
    "Value:  ð‘ ð‘–ð‘›(ð‘’ð‘¥ð‘(0))=0.841 \n",
    "Gradient:  ð‘’ð‘¥ð‘(0)âˆ—ð‘ð‘œð‘ (ð‘’ð‘¥ð‘(0))=0.540"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Multi input, single output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X\n",
      "Value: [[0.]\n",
      " [1.]\n",
      " [2.]]\n",
      "Gradient: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Output:\n",
      "The value is: 3.0\n",
      "The gradient is: [[0. 2. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example R^3 -> R^1\n",
    "\n",
    "# Define a variable with a vector of initial values\n",
    "X = Variable([0,1,2])\n",
    "print(\"Input X\")\n",
    "print(X)\n",
    "\n",
    "# Define a function\n",
    "# Unroll allows us to define an expression using the individual input variables\n",
    "def my_func(X):\n",
    "    x, y, z = X.unroll()\n",
    "    return F.cos(x) + y * z\n",
    "\n",
    "# Variable out is the result of calling function on X\n",
    "out = my_func(X)\n",
    "\n",
    "# Print value and gradient of the output\n",
    "print('\\nOutput:')\n",
    "print('The value is: {}'.format(out.val))\n",
    "print('The gradient is: {}'.format(out.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we initialize \"Variable\" X with a vector of initial values [0, 1, 2]\n",
    "\n",
    "X is thus a vector of 3 input variables, which can be obtained via \"unroll\".\n",
    "We can call these input variables x, y, z\n",
    "\n",
    "We then define a function in terms of the input variables  \n",
    "     $f(x,y,z) = cos(x) + y*z$\n",
    "\n",
    "Lastly, we get the value and the gradient of the function at the initial point:  \n",
    "Value:  \n",
    "$cos(0) + 1*2 = 3$  \n",
    "Gradient:  \n",
    "$ \\frac{\\partial{f}}{\\partial{x}} = -sin(x) = 0$  \n",
    "$ \\frac{\\partial{f}}{\\partial{y}} = z = 2 $  \n",
    "$ \\frac{\\partial{f}}{\\partial{z}} = y = 1 $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Multi input, multi output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X\n",
      "Value: [[0.]\n",
      " [1.]\n",
      " [2.]]\n",
      "Gradient: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "\n",
      "Output:\n",
      "The value is: \n",
      "[[0.84147098]\n",
      " [4.76219569]\n",
      " [0.30103   ]]\n",
      "The gradient is: \n",
      "[[0.54030231 0.         0.        ]\n",
      " [0.         1.         3.62686041]\n",
      " [0.         0.         0.21714724]]\n"
     ]
    }
   ],
   "source": [
    "# Example R^3 -> R^3\n",
    "\n",
    "# Define a variable with a vector of initial values\n",
    "X = Variable([0,1,2])\n",
    "print(\"Input X\")\n",
    "print(X)\n",
    "\n",
    "# Define a function\n",
    "# Unroll allows us to define an expression using the individual input variables\n",
    "# Concat is used to build the output vector of the function\n",
    "def my_func3(X):\n",
    "    x, y, z = X.unroll()\n",
    "    o1 = F.sin(F.exp(x))\n",
    "    o2 = y + F.cosh(z)\n",
    "    o3 = F.Log(10)(z)\n",
    "    out_X = F.concat([o1, o2, o3])\n",
    "    return out_X\n",
    "\n",
    "# Variable out is the result of calling function on X\n",
    "out = my_func3(X)\n",
    "\n",
    "# Print value and gradient of the output\n",
    "print('\\nOutput:')\n",
    "print('The value is: \\n{}'.format(out.val))\n",
    "print('The gradient is: \\n{}'.format(out.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, users of our package may be interested in appyling it to solve optimization problems. As such, we provide an example of how our package could be used to implement the Newton-Raphson method for root finding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current x: 4.0\n",
      "Current x: 3.3333333333333335\n",
      "Current x: 2.888888888888889\n",
      "Current x: 2.5925925925925926\n",
      "Current x: 2.3950617283950617\n",
      "Current x: 2.263374485596708\n",
      "Current x: 2.1755829903978054\n",
      "Current x: 2.11705532693187\n",
      "Current x: 2.078036884621247\n",
      "Current x: 2.052024589747498\n",
      "Current x: 2.034683059831665\n",
      "Current x: 2.023122039887777\n",
      "Current x: 2.015414693258518\n",
      "Current x: 2.0102764621723455\n",
      "Current x: 2.0068509747815635\n",
      "Current x: 2.0045673165210425\n",
      "The root of the function is: 2.0045673165210425\n"
     ]
    }
   ],
   "source": [
    "def newtons_method(function, guess, epsilon):\n",
    "    x = Variable(guess)\n",
    "    f = function(x)\n",
    "    i = 0\n",
    "    max_out = False\n",
    "    while abs(f.val) >= epsilon and max_out == False:\n",
    "        x = x - f.val / f.grad\n",
    "        f = function(x)\n",
    "        print('Current x: {}'.format(x.val))\n",
    "        i += 1\n",
    "        if i >= 10000:\n",
    "            max_out = True\n",
    "    print('The root of the function is: {}'.format(x.val))\n",
    "            \n",
    "\n",
    "def my_func(x):\n",
    "    return 5*(x-2)**3\n",
    "\n",
    "guess = 5\n",
    "epsilon = 0.000001\n",
    "\n",
    "newtons_method(my_func, guess, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Directory Structure:\n",
    "\n",
    "We have our main implementation stored in the autodiff directory. This is where modules for our implementation of the forward mode of AD are located. The autodiff directory has a subdirectory containing tests.\n",
    "\n",
    "```\n",
    "cs207-FinalProject/\n",
    "    docs/\n",
    "        documentation.ipynb\n",
    "        milestone1.ipynb\n",
    "        milestone2.ipynb     \n",
    "        demo/\n",
    "            demo.py\n",
    "            demo_newtons_method.py\n",
    "    autodiff/\n",
    "        __init__.py\n",
    "        function.py\n",
    "        optim.py\n",
    "        variable.py\n",
    "        utils.py     \n",
    "        tests/\n",
    "            __init__.py\n",
    "            test_function.py\n",
    "            test_reverse_function.py\n",
    "            test_reverse_variable.py\n",
    "            test_optim.py\n",
    "            test_variable.py\n",
    "            test_utils.py\n",
    "    demo.ipynb\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    \n",
    "```\n",
    "##### Basic Modules:\n",
    "We included four modules in the autodiff directory as shown above. \n",
    "The function module contains our function class as well as the implementation of our elementary functions that constitute the computational graph. The variable module has our Variable and ReverseVariable class as well as the implementation of all the basic operations (addition, multiplication, subtraction, division, power). The Varibale class serves as the information flow within the computational graph for forward mode, and the ReverseVariable class accomplishes the same purpose for reverse mode. The utils module contains functions to be called by the variable class to make sure the input is of the correct type for the definition of a variable. The optim module has a superclass Optimizer, and its subclasses GradientDescent, RMSProp and Adam. They use automatic differentiations to find local critical points.\n",
    "\n",
    "##### Testing:\n",
    "\n",
    "Our test suite lives within the autodiff directory (see above). Additionally for continuous integration and code coverage we utilize both TravisCI and CodeCov. We set up basic functionality for TravisCI and CodeCov for our repository.\n",
    "\n",
    "Tests can be run simply via:  \n",
    "    \n",
    "    pytest\n",
    "    \n",
    "(or optional-for a more detailed coverage)\n",
    "\n",
    "    pytest --doctest-modules --cov --cov-report term-missing\n",
    "\n",
    "##### Distribution and Packaging:\n",
    "\n",
    "We distributed our package using PyPI (and upload with Twine). The user is able to install our package via the command line: \"pip install autodiff-ADdictedtoCS\"\n",
    "\n",
    "Optionally, the user can manually install our package by downloading our github repo from https://github.com/ADdictedtoCS/cs207-FinalProject.git , setting the current directory to the directory containing requirements.txt, and running \"pip install -r requirements.txt\". Before doing this, the user may want to set up a virtual enviroment using virtualenv. \n",
    "\n",
    "For more details on precisely how the user can install our package, along with examples of how to use our package, please refer to the *\"How to use autodiff\"* section above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "\n",
    "##### Core Data Structures:\n",
    "\n",
    "For each function defined by users, we create a Directed Acyclic Graph (DAG) data structure, which contains all computing nodes for the forward mode of AD.\n",
    "\n",
    "Our operation is based upon two main concepts: \n",
    "* **variable** carries the information flow through the computational graph. (see below).\n",
    "* **function** implements the elementary functions and constitutes the edges (or nodes, depending on ones's interpretation of the computational graph).\n",
    "\n",
    "Within each node, we compute the value and gradient of the function at that point in the computational graph. Therefore, the lists from all nodes form the computation table in the forward mode of AD.\n",
    "\n",
    "##### Classes:\n",
    "\n",
    "**Variable**  \n",
    "Our core class, which carries the information flow within the computational graph.\n",
    "\n",
    "Variables are initialized at a particular value passed in by the user, and have a default gradient of 1 (the \"seed vector\").\n",
    "\n",
    "All of the basic functionality for algebra involving variables is implemented within the Variable class through operator overloading.\n",
    "\n",
    "For example, we define what addition means for Variables in terms of their values and gradients here:\n",
    "\n",
    "```Python\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Variable):\n",
    "            out_val = self.val + other.val\n",
    "            out_grad = self.grad + other.grad\n",
    "            return Variable(val=out_val, grad=out_grad)\n",
    "        else:\n",
    "            new_val = get_right_shape(other)\n",
    "            out_val = self.val + get_right_shape(other)\n",
    "            out_grad = self.grad\n",
    "            return Variable(val=out_val, grad=out_grad)\n",
    "```\n",
    "\n",
    "In the current implementation, Variable can hold multiple values and gradient values (derivatives of multiple points). We rely on numpy arrays for this functionality to allow Variable hold an array of values and an array for gradient as well. \n",
    "\n",
    "\n",
    "**Function**  \n",
    "When called on a variable, a function returns a new variable with a transformed value and gradient.\n",
    "\n",
    "A function has the methods *get_val* and *get_grad* that denote how the function transforms a variables val and grad. Note that these methods are not implemented for the base class but for the elementary functions which are subclasses of function (see below for more details).\n",
    "\n",
    "The function class is also where we implement the chain rule for calculating gradients:\n",
    "\n",
    "```Python\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Implements the chain rule.\n",
    "        Input: autodiff.Variable type holding a val and grad\n",
    "        Output:  autodiff.Variable type holding the val, grad of the\n",
    "        transformed variable\n",
    "        \"\"\"\n",
    "        out_val = self.get_val(x.val)\n",
    "        out_grad = np.dot(self.get_grad(x.val), x.grad)\n",
    "        return Variable(val=out_val, grad=out_grad)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "##### Important Attributes:\n",
    "\n",
    "In our current implementation, the key attributes for the **Variable** class are:\n",
    "\n",
    "*val* = The value. Can be an int, float, or numpy.array. When a variable is first initialized, this is passed in by the user.\n",
    "\n",
    "*grad* = The gradient. Can be an int, float, or numpy.array. When the variable is first initialized this has a default value of 1 (the \"seed vector\").\n",
    "\n",
    "As mentioned above, functions take in a variable and output a new variable with a transformed value and gradient.\n",
    "\n",
    "The value and gradient of a variable can be obtained by the user by calling .val or .grad on the variable. Additionally, the user can print a variable to see its value and gradient.\n",
    "\n",
    "```Python\n",
    "    # Define a variable with an initial value\n",
    "    x = Variable(5.)\n",
    "    \n",
    "    x.val\n",
    "    >>>> [5.]\n",
    "    \n",
    "    x.grad\n",
    "    >>>> [1.]\n",
    "    \n",
    "    print(x)\n",
    "    >>>> Value: [5.]\n",
    "    >>>> Gradient: [1.]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### External dependencies:\n",
    "External dependencies for our package are listed in requirements.txt\n",
    "\n",
    "The required external dependencies are Numpy and Matplotlib.\n",
    "\n",
    "Required:\n",
    "- Numpy 1.16.4 (for vector operations)\n",
    "- Matplotlib 3.1.1 (for visualization)\n",
    "\n",
    "\n",
    "#### Elementary functions:\n",
    "\n",
    "The elementary functions such as **sin**, **exp**, etc are all implemented as subclasses of **Function**.\n",
    "\n",
    "For each elementary function, we explicitedly define its value and derivative. For example, $e^x$ below:\n",
    "\n",
    "```Python\n",
    "    class Exponent(Function):\n",
    "        \"\"\"Exponential\"\"\"    \n",
    "        def get_val(self, x):\n",
    "            return np.exp(x)\n",
    "    \n",
    "        def get_grad(self, x):\n",
    "            return np.exp(x)\n",
    "```\n",
    "\n",
    "In our current implementation, the elementary functions we have implemented along with their derivatives are as follows:\n",
    "\n",
    "| Elementary Function      |  Derivative            |\n",
    "|--------------------------|------------------------|\n",
    "|        $e^x$             |   $e^x$                |\n",
    "|        $sin(x)$          |   $cos(x)$             |\n",
    "|        $cos(x)$          |   $-sin(x)$            |\n",
    "|        $tan(x)$          |   $\\frac{1}{cos(x)^2}$ | \n",
    "|        $arcsin(x)$       |   $\\frac{1}{\\sqrt{1-x^2}}$ | \n",
    "|        $arccos(x)$       |   $-\\frac{1}{\\sqrt{1-x^2}}$ | \n",
    "|        $arctan(x)$       |   $\\frac{1}{1+x^2}$ | \n",
    "|        $log(x)$          |   $\\frac{1}{x}$ | \n",
    "|        $logistic(x)$     |   $x(1-x)$ | \n",
    "|        $sqrt(x)$         |   $\\frac{1}{2\\sqrt{x}}$ | \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Extention\n",
    "\n",
    "##### Vector Valued and Multivariate Functions:\n",
    "\n",
    "Our immediate next step is to update our package to allow for functions with multiple inputs and outputs. We can accomplish this by reworking our variable class to allow for an array of inputs and to compute the full gradient at each step of the computational graph.\n",
    "\n",
    "Thus, we will also have to update our code to incorporate matrix algebra, for which we plan to rely on the numpy package.\n",
    "\n",
    "Implementing this change will involve the following:\n",
    "\n",
    "a. The initialization of Variable objects will also accept vector-based values, which could be built using the numpy library.  \n",
    "\n",
    "b. The attributes of Variable class, \"val\" and \"grad\", will be vector valued as well.  \n",
    "\n",
    "c. Operators such as \"+\", \"-\", \"\\*\", and \"/\" will need to be redefined. Each operator will now involve performing vector operations. We foresee a primary challenge being implementing vector operations for \"/\" and \"\\*\\*\", as we will have to be extra careful to check for edge-cases such as division by 0.  \n",
    "\n",
    "d. For the \"\\*\" operator, additional work will also need to be done. This operator will need to accept matrix values, and check whether two matrices are of the correct shapes to be multiplied.  \n",
    "\n",
    "e. We will also need to rework the elementary functions to allow for vector-valued inputs.  \n",
    "\n",
    "f. The definition of \"grad\" will change. In the forward mode of autodiff, a variable will record the gradient for each \"root variable\" (variables we create manually). Therefore, we can change \"grad\" as a **dict** of gradients, with one gradient value for each root variable. \n",
    "\n",
    "g. The definition of operators will change. When creating a new variable by performing an operation between two variables, we should calculate the gradient for each root variable. If the gradient of one root variable does not exist for one variable, then it will be 0 for that variable. \n",
    "\n",
    "\n",
    "##### Argument Parser:\n",
    "\n",
    "An additional feature we would like to implement that we believe could improve ease of use of our package for users is an argument parser, which would allow users to input functions as strings.\n",
    "\n",
    "In our current implementation, as shown in the \"How to use autodiff\" section, the user must enter functions by specifically calling elementary functions such as:\n",
    "\n",
    "    F.sin(F.exp(x))\n",
    "    \n",
    "Our argument parser would be built so that users could have the option of entering functions more naturally as strings such as:\n",
    "\n",
    "    \"sin(e^x)\"\n",
    "   \n",
    "We would use regular expressions to implement our argument parser in a way that is robust and accounts for a wide range of possible user inputs.\n",
    "\n",
    "##### Reverse Mode:\n",
    "\n",
    "As mentioned in the \"Software Organization\" section above, we may want to implement the reverse mode of autodifferentiation in the future. This could be a useful addition to our package, because, as mentioned above the reverse mode is more efficient when the number of independent variables is much greater than the number of functions. We would likely create reverse mode as a separate module within our autodiff directory.\n",
    "\n",
    "Several changes should be performed:\n",
    "\n",
    "* Because reverse mode includes a forward pass and a reverse calculation, we may record the graph structrure when we create the final function. In other words, when we create a new variable using a function (or variable operators) from 1 or 2 base variables, the function should record those base variables, and store them is a list or dict for later use.\n",
    "* The definition of gradient for the Variable class could be optimized. Because we only need to record at most 2 gradients (one for each child base variable), we can use a tuple or just use two attributes to record the gradient within the Variable class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
