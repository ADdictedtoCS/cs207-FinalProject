{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Demo of Autodiff\n",
    "\n",
    "#### (Note this demo is here for ease of access to users and similar demos can be found in milestone2.ipynb)\n",
    "\n",
    "\n",
    "See below for a basic example of how the user can interact with our package:\n",
    "\n",
    "The basic steps are:\n",
    "0. Import package\n",
    "1. Create a variable instantiated at an initial value \n",
    "    - Below we create variable x with a value of 0\n",
    "2. Define a function \n",
    "    - Functions take variables as inputs and return new variables with updated value and gradient\n",
    "    - Below we define the function $f(x) := sin(exp(x))$\n",
    "3. Call the function and get the value and gradient of the resulting variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%cd cs207-FinalProject\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x Value: [0.]\n",
      "Gradient: [[1.]]\n",
      "Output z Value: [0.84147098]\n",
      "Gradient: 0.5403023058681398\n",
      "The value is: [0.84147098]\n",
      "The gradient is: 0.5403023058681398\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from autodiff.variable import Variable\n",
    "import autodiff.function as F\n",
    "\n",
    "# Define a variable with an initial value\n",
    "x = Variable(0.)\n",
    "print(\"Input x\", x)\n",
    "\n",
    "# Define a function\n",
    "def my_func(x):\n",
    "    return F.sin(F.exp(x))\n",
    "\n",
    "# Variable z is the result of calling function on x\n",
    "z = my_func(x)\n",
    "\n",
    "# Get value and gradient of z\n",
    "print(\"Output z\", z)\n",
    "\n",
    "# Alternatively, with direct access to the value and gradient attributes.\n",
    "print('The value is: {}'.format(z.val))\n",
    "print('The gradient is: {}'.format(z.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Implementation of Newton's Method\n",
    "As seen above, autodiff provides us with a natural way to define a specified function, by composition of elementary functions.  \n",
    "\n",
    "Furthermore, autodiff objects can be used to implement higher level algorithms. For example, users of our package may be interested in appyling it to solve optimization problems. As such, we provide an example of how our package could be used to implement the Newton-Raphson method for root finding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[27.]]\n",
      "Initializing the gradient with [[135.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[12.]]\n",
      "Initializing the gradient with [[60.]]\n",
      "Current x: [4.]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[5.33333333]]\n",
      "Initializing the gradient with [[26.66666667]]\n",
      "Current x: [3.33333333]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[2.37037037]]\n",
      "Initializing the gradient with [[11.85185185]]\n",
      "Current x: [2.88888889]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.05349794]]\n",
      "Initializing the gradient with [[5.26748971]]\n",
      "Current x: [2.59259259]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.46822131]]\n",
      "Initializing the gradient with [[2.34110654]]\n",
      "Current x: [2.39506173]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.20809836]]\n",
      "Initializing the gradient with [[1.04049179]]\n",
      "Current x: [2.26337449]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.09248816]]\n",
      "Initializing the gradient with [[0.4624408]]\n",
      "Current x: [2.17558299]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.04110585]]\n",
      "Initializing the gradient with [[0.20552924]]\n",
      "Current x: [2.11705533]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.01826927]]\n",
      "Initializing the gradient with [[0.09134633]]\n",
      "Current x: [2.07803688]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.00811967]]\n",
      "Initializing the gradient with [[0.04059837]]\n",
      "Current x: [2.05202459]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.00360874]]\n",
      "Initializing the gradient with [[0.01804372]]\n",
      "Current x: [2.03468306]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.00160389]]\n",
      "Initializing the gradient with [[0.00801943]]\n",
      "Current x: [2.02312204]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.00071284]]\n",
      "Initializing the gradient with [[0.00356419]]\n",
      "Current x: [2.01541469]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.00031682]]\n",
      "Initializing the gradient with [[0.00158409]]\n",
      "Current x: [2.01027646]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[0.00014081]]\n",
      "Initializing the gradient with [[0.00070404]]\n",
      "Current x: [2.00685097]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[1.]]\n",
      "Initializing the gradient with [[6.25811406e-05]]\n",
      "Initializing the gradient with [[0.00031291]]\n",
      "Current x: [2.00456732]\n",
      "The root of the function is: [2.00456732]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from autodiff.variable import Variable\n",
    "import autodiff.function as F\n",
    "\n",
    "\n",
    "def newtons_method(function, guess, epsilon):\n",
    "    x = Variable(guess)\n",
    "    f = function(x)\n",
    "    i = 0\n",
    "    max_out = False\n",
    "    while abs(f.val) >= epsilon and max_out == False:\n",
    "        x = x - f.val / f.grad\n",
    "        f = function(x)\n",
    "        print('Current x: {}'.format(x.val))\n",
    "        i += 1\n",
    "        if i >= 10000:\n",
    "            max_out = True\n",
    "    print('The root of the function is: {}'.format(x.val))\n",
    "            \n",
    "\n",
    "def my_func(x):\n",
    "    return 5*(x-2)**3\n",
    "\n",
    "guess = 5\n",
    "epsilon = 0.000001\n",
    "\n",
    "newtons_method(my_func, guess, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What does the full variable look like Value: [ 1.5  5.  10. ]\n",
      "Gradient: [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]] \n",
      "\n",
      "What do the unrolled variables look like Value: [1.5]\n",
      "Gradient: [[1. 0. 0.]] Value: [5.]\n",
      "Gradient: [[0. 1. 0.]] Value: [10.]\n",
      "Gradient: [[0. 0. 1.]] \n",
      "\n",
      "Our variable Value: [6.26535126]\n",
      "Gradient: [[5.48168907 0.95892427 0.        ]] \n",
      "\n",
      "Expected value 6.265351255801291\n",
      "Expected gradients 5.4816890703380645 0.9589242746631385 0\n"
     ]
    }
   ],
   "source": [
    "#Multivariable demo. Please play around to see the edge cases. If we want a function R^3->R.\n",
    "X = Variable(np.array([1.5 ,5 ,10]))\n",
    "x,y,z = F.unroll(X)\n",
    "\n",
    "print('What does the full variable look like', X, '\\n')\n",
    "print('What do the unrolled variables look like', x,y,z, '\\n')\n",
    "#Operations\n",
    "out = F.exp(x) + F.cos(y)\n",
    "out += x\n",
    "#=============\n",
    "#Check whether it matches what we hoped ?\n",
    "#===========\n",
    "print('Our final variable', out, '\\n')\n",
    "print('Expected value', np.exp(X.val[0]) + np.cos(X.val[1]) + X.val[0])\n",
    "print('Expected gradients', np.exp(X.val[0]) + 1, -np.sin(X.val[1]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [75.]\n",
      "Gradient: [[50.  15.   7.5]]\n"
     ]
    }
   ],
   "source": [
    "#Mult.\n",
    "mul = x*y*z \n",
    "print(mul)\n",
    "print('Check', y.val*z.val, x.val*z.val, x.val*y.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodiff.variable import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject/autodiff/function.py:379: UserWarning: Using the element-wise composition. Beta version. Not entirely tested.\n",
      "  \"Using the element-wise composition. Beta version. Not entirely tested.\")\n",
      "/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject/autodiff/function.py:392: UserWarning: Using the element-wise composition. Beta version. Not entirely tested.\n",
      "  \"Using the element-wise composition. Beta version. Not entirely tested.\")\n",
      "/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject/autodiff/function.py:405: UserWarning: Using the element-wise composition. Beta version. Not entirely tested.\n",
      "  \"Using the element-wise composition. Beta version. Not entirely tested.\")\n",
      "/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject/autodiff/function.py:418: UserWarning: Using the element-wise composition. Beta version. Not entirely tested.\n",
      "  \"Using the element-wise composition. Beta version. Not entirely tested.\")\n"
     ]
    }
   ],
   "source": [
    "import autodiff.function as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value:\n",
      "[[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [54.59815003]]\n",
      "Gradient:\n",
      "[[ 2.71828183  0.          0.        ]\n",
      " [ 0.          7.3890561   0.        ]\n",
      " [ 0.          0.         54.59815003]]\n",
      "\n",
      " Value:\n",
      "[[ 2.71828183]\n",
      " [ 7.3890561 ]\n",
      " [54.59815003]]\n",
      "Gradient:\n",
      "[[ 2.71828183  0.          0.        ]\n",
      " [ 0.          7.3890561   0.        ]\n",
      " [ 0.          0.         54.59815003]]\n"
     ]
    }
   ],
   "source": [
    "#For some functions, namely exp, sin, cos, tan, we can directly use \n",
    "#some elementary-wise operators, the followig way:\n",
    "X = Variable([1,2,4])\n",
    "ew_W = F.ew_exp(X)\n",
    "print(ew_W)\n",
    "#===============\n",
    "# By unroll and concatenating\n",
    "#===============\n",
    "x, y, z = X.unroll()\n",
    "out_x, out_y, out_z = F.exp(x), F.exp(y), F.exp(z)\n",
    "out_X = F.concat([out_x, out_y, out_z])\n",
    "print('\\n', out_X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
