{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Demo of Autodiff\n",
    "\n",
    "#### (Note this demo is here for ease of access to users and similar demos can be found in milestone2.ipynb)\n",
    "\n",
    "\n",
    "See below for a basic example of how the user can interact with our package:\n",
    "\n",
    "The basic steps are:\n",
    "0. Import package\n",
    "1. Create a variable instantiated at an initial value \n",
    "    - Below we create variable x with a value of 0\n",
    "2. Define a function \n",
    "    - Functions take variables as inputs and return new variables with updated value and gradient\n",
    "    - Below we define the function $f(x) := sin(exp(x))$\n",
    "3. Call the function and get the value and gradient of the resulting variable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject\n"
    },
    {
     "data": {
      "text/plain": "'/Users/theoguenais/Desktop/Harvard/Harvard-Classes/CS207/cs207-FinalProject'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%cd cs207-FinalProject\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input x Value: [0.]\nGradient: [[1.]]\nOutput z Value: [0.84147098]\nGradient: 0.5403023058681398\nThe value is: [0.84147098]\nThe gradient is: 0.5403023058681398\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from autodiff.variable import Variable\n",
    "import autodiff.function as F\n",
    "\n",
    "# Define a variable with an initial value\n",
    "x = Variable(0.)\n",
    "print(\"Input x\", x)\n",
    "\n",
    "# Define a function\n",
    "def my_func(x):\n",
    "    return F.sin(F.exp(x))\n",
    "\n",
    "# Variable z is the result of calling function on x\n",
    "z = my_func(x)\n",
    "\n",
    "# Get value and gradient of z\n",
    "print(\"Output z\", z)\n",
    "\n",
    "# Alternatively, with direct access to the value and gradient attributes.\n",
    "print('The value is: {}'.format(z.val))\n",
    "print('The gradient is: {}'.format(z.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Implementation of Newton's Method\n",
    "As seen above, autodiff provides us with a natural way to define a specified function, by composition of elementary functions.  \n",
    "\n",
    "Furthermore, autodiff objects can be used to implement higher level algorithms. For example, users of our package may be interested in appyling it to solve optimization problems. As such, we provide an example of how our package could be used to implement the Newton-Raphson method for root finding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Initializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[27.]]\nInitializing the gradient with [[135.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[12.]]\nInitializing the gradient with [[60.]]\nCurrent x: [4.]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[5.33333333]]\nInitializing the gradient with [[26.66666667]]\nCurrent x: [3.33333333]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[2.37037037]]\nInitializing the gradient with [[11.85185185]]\nCurrent x: [2.88888889]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.05349794]]\nInitializing the gradient with [[5.26748971]]\nCurrent x: [2.59259259]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.46822131]]\nInitializing the gradient with [[2.34110654]]\nCurrent x: [2.39506173]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.20809836]]\nInitializing the gradient with [[1.04049179]]\nCurrent x: [2.26337449]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.09248816]]\nInitializing the gradient with [[0.4624408]]\nCurrent x: [2.17558299]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.04110585]]\nInitializing the gradient with [[0.20552924]]\nCurrent x: [2.11705533]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.01826927]]\nInitializing the gradient with [[0.09134633]]\nCurrent x: [2.07803688]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.00811967]]\nInitializing the gradient with [[0.04059837]]\nCurrent x: [2.05202459]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.00360874]]\nInitializing the gradient with [[0.01804372]]\nCurrent x: [2.03468306]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.00160389]]\nInitializing the gradient with [[0.00801943]]\nCurrent x: [2.02312204]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.00071284]]\nInitializing the gradient with [[0.00356419]]\nCurrent x: [2.01541469]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.00031682]]\nInitializing the gradient with [[0.00158409]]\nCurrent x: [2.01027646]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[0.00014081]]\nInitializing the gradient with [[0.00070404]]\nCurrent x: [2.00685097]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[1.]]\nInitializing the gradient with [[6.25811406e-05]]\nInitializing the gradient with [[0.00031291]]\nCurrent x: [2.00456732]\nThe root of the function is: [2.00456732]\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from autodiff.variable import Variable\n",
    "import autodiff.function as F\n",
    "\n",
    "\n",
    "def newtons_method(function, guess, epsilon):\n",
    "    x = Variable(guess)\n",
    "    f = function(x)\n",
    "    i = 0\n",
    "    max_out = False\n",
    "    while abs(f.val) >= epsilon and max_out == False:\n",
    "        x = x - f.val / f.grad\n",
    "        f = function(x)\n",
    "        print('Current x: {}'.format(x.val))\n",
    "        i += 1\n",
    "        if i >= 10000:\n",
    "            max_out = True\n",
    "    print('The root of the function is: {}'.format(x.val))\n",
    "            \n",
    "\n",
    "def my_func(x):\n",
    "    return 5*(x-2)**3\n",
    "\n",
    "guess = 5\n",
    "epsilon = 0.000001\n",
    "\n",
    "newtons_method(my_func, guess, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "What does the full variable look like Value: [ 1.5  5.  10. ]\nGradient: [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]] \n\nWhat do the unrolled variables look like Value: [1.5]\nGradient: [[1. 0. 0.]] Value: [5.]\nGradient: [[0. 1. 0.]] Value: [10.]\nGradient: [[0. 0. 1.]] \n\nOur variable Value: [6.26535126]\nGradient: [[5.48168907 0.95892427 0.        ]] \n\nExpected value 6.265351255801291\nExpected gradients 5.4816890703380645 0.9589242746631385 0\n"
    }
   ],
   "source": [
    "#Multivariable demo. Please play around to see the edge cases. If we want a function R^3->R.\n",
    "X = Variable(np.array([1.5 ,5 ,10]))\n",
    "x,y,z = F.unroll(X)\n",
    "\n",
    "print('What does the full variable look like', X, '\\n')\n",
    "print('What do the unrolled variables look like', x,y,z, '\\n')\n",
    "#Operations\n",
    "out = F.exp(x) + F.cos(y)\n",
    "out += x\n",
    "#=============\n",
    "#Check whether it matches what we hoped ?\n",
    "#===========\n",
    "print('Our final variable', out, '\\n')\n",
    "print('Expected value', np.exp(X.val[0]) + np.cos(X.val[1]) + X.val[0])\n",
    "print('Expected gradients', np.exp(X.val[0]) + 1, -np.sin(X.val[1]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Value: [75.]\nGradient: [[50.  15.   7.5]]\n"
    }
   ],
   "source": [
    "#Mult.\n",
    "mul = x*y*z \n",
    "print(mul)\n",
    "print('Check', y.val*z.val, x.val*z.val, x.val*y.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}